{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import string\n",
    "import random \n",
    "import joblib \n",
    "\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from hyperopt import fmin, tpe, rand, hp, STATUS_OK, space_eval\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "\n",
    "from sklearn.metrics import matthews_corrcoef, make_scorer, confusion_matrix, f1_score, recall_score, precision_score, roc_auc_score, accuracy_score, balanced_accuracy_score, log_loss\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "from imblearn.metrics import geometric_mean_score\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#globals\n",
    "DATA_PATH = '../studied data'\n",
    "VALIDATION_PROCESS = \"CRDP\"\n",
    "DATASET = \"jira\"\n",
    "RESULTS_PATH = '../results'\n",
    "RANDOM_STATE = 42\n",
    "NRUNS = 10\n",
    "CV = 5\n",
    "MAX_EVAL = 150\n",
    "KFOLD = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "PARAMETERS_GRID = {\n",
    "\n",
    "    \n",
    "    'DT' :{\n",
    "        \"default\" : DecisionTreeClassifier(class_weight='balanced', random_state = RANDOM_STATE), \n",
    "        'grid': {\n",
    "                'criterion': Categorical(['gini', 'entropy', 'log_loss']),\n",
    "                'splitter': Categorical(['best', 'random']),\n",
    "                'max_features': Categorical(['sqrt', 'log2', None]),\n",
    "                'min_samples_split': Integer(2, 20),\n",
    "                'min_samples_leaf': Integer(1, 8),\n",
    "                'max_depth': Categorical([2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, None]),\n",
    "                'ccp_alpha': Real(0.0, 1.0),\n",
    "                'min_weight_fraction_leaf': Real(0.0, 0.5),  # Minimum weighted fraction of sum of total weights\n",
    "                'max_leaf_nodes': Integer(2, 100),  # Maximum number of leaf nodes\n",
    "                'min_impurity_decrease': Real(0.0, 0.5),  # Minimum impurity decrease for node splitting\n",
    "               \n",
    "        }\n",
    "    }\n",
    "    \n",
    "\n",
    "}\n",
    "\n",
    "'''\n",
    "'RF': {\n",
    "        'default': RandomForestClassifier(class_weight='balanced', n_jobs=-1, random_state = RANDOM_STATE),\n",
    "        'grid': {\n",
    "            'ccp_alpha': Real( 0.0, 1.0),\n",
    "            'criterion': Categorical(['gini', 'entropy', 'log_loss']),\n",
    "            'max_depth': Categorical([2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, None]),\n",
    "            'max_features':  Categorical( [\"sqrt\", \"log2\", None]), \n",
    "            'n_estimators': Categorical([10 * i for i in np.arange(1, 100)]),\n",
    "            'min_samples_split': Real( 0.01, 0.5),  # Minimum samples required to split a node\n",
    "            'min_samples_leaf': Real(0.01, 0.5),  # Minimum samples required for a leaf node\n",
    "            'bootstrap': Categorical([True, False]),  # Bootstrap samples\n",
    "            'min_weight_fraction_leaf': Real(0.0, 0.5),  # Minimum weighted fraction of sum of total weights\n",
    "            'max_leaf_nodes': Integer(2, 100),  # Maximum number of leaf nodes            \n",
    "        }\n",
    "    },\n",
    "    ,\n",
    "    'KNN': {\n",
    "        'default': KNeighborsClassifier( n_jobs= -1),\n",
    "        'grid': {\n",
    "            'n_neighbors':Integer(1, 15),\n",
    "            'weights': Categorical(['uniform', 'distance']),\n",
    "            'algorithm': Categorical(['auto', 'ball_tree', 'kd_tree', 'brute']),\n",
    "            'p':Integer(1, 6),\n",
    "            'leaf_size': Categorical([20, 30, 40]),\n",
    "            'metric': Categorical(['euclidean', 'manhattan', 'chebyshev', 'minkowski'])\n",
    "          \n",
    "        }\n",
    "    },\n",
    "\n",
    "   \n",
    "  \n",
    "    \n",
    "    }\n",
    "    \n",
    "    \n",
    "'''\n",
    "    \n",
    "\n",
    "SEARCH = GridSearchCV\n",
    "\n",
    "FEATURES = [\n",
    "    'CountDeclMethodPrivate', 'AvgLineCode', 'CountLine',\n",
    "       'MaxCyclomatic', 'CountDeclMethodDefault', 'AvgEssential',\n",
    "       'CountDeclClassVariable', 'SumCyclomaticStrict', 'AvgCyclomatic',\n",
    "       'AvgLine', 'CountDeclClassMethod', 'AvgLineComment',\n",
    "       'AvgCyclomaticModified', 'CountDeclFunction', 'CountLineComment',\n",
    "       'CountDeclClass', 'CountDeclMethod', 'SumCyclomaticModified',\n",
    "       'CountLineCodeDecl', 'CountDeclMethodProtected',\n",
    "       'CountDeclInstanceVariable', 'MaxCyclomaticStrict',\n",
    "       'CountDeclMethodPublic', 'CountLineCodeExe', 'SumCyclomatic',\n",
    "       'SumEssential', 'CountStmtDecl', 'CountLineCode', 'CountStmtExe',\n",
    "       'RatioCommentToCode', 'CountLineBlank', 'CountStmt',\n",
    "       'MaxCyclomaticModified', 'CountSemicolon', 'AvgLineBlank',\n",
    "       'CountDeclInstanceMethod', 'AvgCyclomaticStrict',\n",
    "       'PercentLackOfCohesion', 'MaxInheritanceTree', 'CountClassDerived',\n",
    "       'CountClassCoupled', 'CountClassBase', 'CountInput_Max',\n",
    "       'CountInput_Mean', 'CountInput_Min', 'CountOutput_Max',\n",
    "       'CountOutput_Mean', 'CountOutput_Min', 'CountPath_Max',\n",
    "       'CountPath_Mean', 'CountPath_Min', 'MaxNesting_Max', 'MaxNesting_Mean',\n",
    "       'MaxNesting_Min', 'COMM', 'ADEV', 'DDEV', 'Added_lines', 'Del_lines',\n",
    "       'OWN_LINE', 'OWN_COMMIT', 'MINOR_COMMIT', 'MINOR_LINE', 'MAJOR_COMMIT',\n",
    "       'MAJOR_LINE'\n",
    "]\n",
    "TARGET = 'RealBug'\n",
    "\n",
    "SCORERS = [ make_scorer(matthews_corrcoef), 'neg_log_loss', make_scorer(geometric_mean_score), 'accuracy', 'balanced_accuracy', 'roc_auc', 'f1', 'f1_weighted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimze(X, y, model, metrics='MCC', opt_algo='TPE', max_eval = MAX_EVAL, cv = KFOLD): \n",
    "    def run_optimizer(param): \n",
    "        clf = copy.deepcopy(model['default'])\n",
    "        clf.set_params(**param)\n",
    "        metrics_ = []\n",
    "        for train_index, val_index in cv.split(X, y):\n",
    "            train_X, val_X = X.iloc[train_index], X.iloc[val_index]\n",
    "            train_y, val_y = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "\n",
    "            clf.fit(train_X, train_y)\n",
    "\n",
    "            pred_y = clf.predict(val_X)\n",
    "            prob_y = clf.predict_proba(val_X)[:, 1]\n",
    "\n",
    "            if metrics == 'MCC':\n",
    "                metrics_.append(matthews_corrcoef(val_y, pred_y))\n",
    "            \n",
    "            if metrics == 'F1':\n",
    "                metrics_.append(f1_score(val_y, pred_y))\n",
    "\n",
    "            if metrics == 'G':\n",
    "                metrics_.append(geometric_mean_score(val_y, pred_y))\n",
    "            \n",
    "            if metrics == 'accuracy':\n",
    "                metrics_.append(accuracy_score(val_y, pred_y))\n",
    "            \n",
    "            if metrics == 'balanced_accuracy':\n",
    "                metrics_.append(balanced_accuracy_score(val_y, pred_y))\n",
    "            \n",
    "            if metrics == 'roc_auc': \n",
    "                metrics_.append(roc_auc_score(val_y, pred_y))\n",
    "            \n",
    "            if metrics == 'neg_log_loss': \n",
    "                metrics_.append(log_loss(val_y, prob_y))\n",
    "        \n",
    "        if metrics != 'neg_log_loss':\n",
    "            return {\n",
    "                'loss': 1 - np.mean(metrics_),\n",
    "                'status': STATUS_OK\n",
    "            }\n",
    "        else: \n",
    "            return {\n",
    "                'loss': np.mean(metrics_),\n",
    "                'status': STATUS_OK\n",
    "            }\n",
    "    param_space = model['grid']\n",
    "\n",
    "    if opt_algo == 'RAND':\n",
    "        best = fmin(run_optimizer, param_space, algo=rand.suggest, max_evals=max_eval, show_progressbar=False)\n",
    "\n",
    "    elif opt_algo == 'TPE':\n",
    "        best = fmin(run_optimizer, param_space, algo=tpe.suggest, max_evals=max_eval, show_progressbar=False)\n",
    "\n",
    "    else:\n",
    "        print('PLEASE SET YOUR OPTIMIZATION ALGORITHM !!!')\n",
    "\n",
    "    params = space_eval(param_space, best)\n",
    "    model_tune = copy.deepcopy(model[\"default\"]).set_params(**params)\n",
    "\n",
    "\n",
    "    model_tune.fit(X, y)\n",
    "\n",
    "    return model_tune, params\n",
    "\n",
    "def compute_all_models_diversities(all_models_predictions, y_true, models, scorers = SCORERS):\n",
    "    transposed_predictions = {}\n",
    "    for imodel in models: \n",
    "        for scorer in scorers: \n",
    "            transposed_predictions[imodel + '-' + str(scorer)] = all_models_predictions[imodel][str(scorer)]\n",
    "    \n",
    "    return compute_model_diversities(transposed_predictions, y_true, scorers=list(transposed_predictions.keys()))\n",
    "\n",
    "\n",
    "def compute_model_diversities(predictions, y_true, scorers = SCORERS): \n",
    "    results = []\n",
    "    for index_i in range(len(scorers)): \n",
    "        for index_j in range(len(scorers)): \n",
    "            scorer_i = str(scorers[index_i])\n",
    "            scorer_j = str(scorers[index_j])\n",
    "            diversity_value = compute_diversity(predictions[scorer_i],predictions[scorer_j], y_true)\n",
    "            results.append({\n",
    "                'scorer_1': scorer_i , \n",
    "                'scorer_2': scorer_j, \n",
    "                'diversity': diversity_value\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def compute_diversity(classifier_1_predictions, classifier_2_predictions, y):\n",
    "    classifier_1_correctly_classified_instances = np.where(np.equal(classifier_1_predictions, y))[0]\n",
    "    classifier_1_notcorrect_classified_instances = np.where(np.not_equal(classifier_1_predictions, y))[0]\n",
    "\n",
    "    classifier_2_correctly_classified_instances = np.where(np.equal(classifier_2_predictions, y))[0]\n",
    "    classifier_2_notcorrect_classified_instances = np.where(np.not_equal(classifier_2_predictions, y))[0]\n",
    "\n",
    "    N_1_0 = len(set(classifier_1_correctly_classified_instances).intersection(set(classifier_2_notcorrect_classified_instances)))\n",
    "    N_0_1 = len(set(classifier_2_correctly_classified_instances).intersection(set(classifier_1_notcorrect_classified_instances)))\n",
    "\n",
    "    N_1_1 = len(set(classifier_1_correctly_classified_instances).intersection(set(classifier_2_correctly_classified_instances)))\n",
    "    N_0_0 = len(set(classifier_1_notcorrect_classified_instances).intersection(set(classifier_2_notcorrect_classified_instances)))\n",
    "\n",
    "    return (N_1_0 + N_0_1)/(N_1_0 + N_0_1 + N_1_1 + N_0_0)\n",
    "\n",
    "\n",
    "def evaluate_model_predictions(y_true, y_pred, y_prob): \n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n",
    "\n",
    "    res ={\n",
    "        'AUC': roc_auc_score(y_true, y_prob), \n",
    "        'MCC':matthews_corrcoef(y_true, y_pred),\n",
    "        'G' : geometric_mean_score(y_true, y_pred), \n",
    "        'f1' : f1_score(y_true, y_pred),\n",
    "        'tpr': recall_score(y_true, y_pred, pos_label=1),\n",
    "        'tnr' : recall_score(y_true, y_pred,pos_label=0),\n",
    "        'precision': precision_score(y_true, y_pred), \n",
    "        'fpr': 1 - recall_score(y_true, y_pred,pos_label=0),\n",
    "        'fnr': 1 - recall_score(y_true, y_pred,pos_label=1),\n",
    "        'tp' : tp, \n",
    "        'tn': tn, \n",
    "        'fp': fp, \n",
    "        'fn': fn\n",
    "    }\n",
    "    return res\n",
    "\n",
    "def random_string(length):\n",
    "    pool = string.ascii_letters + string.digits\n",
    "    return ''.join(random.choice(pool) for i in range(length))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "\n",
    "experiment_hash = random_string(32)\n",
    "print('starting experiment:', experiment_hash)\n",
    "os.makedirs(os.path.join(RESULTS_PATH, 'experiment_' + experiment_hash), exist_ok=True)\n",
    "experiment_models_path = os.path.join(RESULTS_PATH, 'experiment_' + experiment_hash, 'MODELS')\n",
    "figures_folder = os.path.join(RESULTS_PATH, 'experiment_' + experiment_hash,'Diversity_figures')\n",
    "os.makedirs(experiment_models_path, exist_ok=True)\n",
    "os.makedirs(figures_folder, exist_ok=True)\n",
    "\n",
    "experiment_metadata = {\n",
    "    'PARAMETERS_GRID': PARAMETERS_GRID, \n",
    "    'DATASET': DATASET, \n",
    "    'VALIDATION_PROCESS': VALIDATION_PROCESS, \n",
    "    'CV': CV, \n",
    "    'SEARCH':SEARCH, \n",
    "    'SCORES': SCORERS,\n",
    "    'IS_DONE': False, \n",
    "}\n",
    "all_models_train_predictions = {}\n",
    "all_models_test_predictions = {}\n",
    "for file in os.listdir(os.path.join(DATA_PATH, VALIDATION_PROCESS, DATASET)): \n",
    "    \n",
    "    \n",
    "\n",
    "    if not(\".csv\" in file):\n",
    "        continue\n",
    "    if not (\"train\" in file): \n",
    "        continue\n",
    "    \n",
    "    print('Working on:', file)\n",
    "\n",
    "    file_models_train_predictions = {}\n",
    "    file_models_test_predictions = {}\n",
    "    project_performance = []\n",
    "\n",
    "    train_data = pd.read_csv(os.path.join(DATA_PATH, VALIDATION_PROCESS, DATASET, file))\n",
    "    test_data = pd.read_csv(os.path.join(DATA_PATH, VALIDATION_PROCESS, DATASET, file.replace(\"train\", 'test')))\n",
    "\n",
    "    X_train, y_train = train_data[FEATURES], train_data[TARGET] \n",
    "    X_test, y_test = test_data[FEATURES], test_data[TARGET] \n",
    "\n",
    "    for run in range(NRUNS):\n",
    "        print(\"Run:\", run)\n",
    "    \n",
    "        for model_name, model_data in PARAMETERS_GRID.items():\n",
    "\n",
    "            model_train_predictions = {}\n",
    "            model_test_predictions = {}\n",
    "            model_performance = []\n",
    "            print('******** Model:',model_name)\n",
    "            for scorer in SCORERS:\n",
    "\n",
    "                print('*****************', str(scorer))\n",
    "\n",
    "                search = BayesSearchCV(estimator= model_data['default'],\n",
    "                                        search_spaces=model_data['grid'], cv=KFOLD, scoring=scorer, refit=True, n_jobs= -1, \n",
    "                                        verbose=0, random_state=RANDOM_STATE, n_iter = MAX_EVAL)\n",
    "                search.fit(X_train, y_train)\n",
    "                #final_model, best_params = optimze(X_train, y_train, model_data, scorer)\n",
    "                joblib.dump(search, os.path.join(experiment_models_path, f'{file}_-_{model_name}_-_run-{str(run)}_-_{str(scorer)}.joblib'))\n",
    "                print(\"best parameters:\", search.best_params_)\n",
    "                y_train_pred = search.predict(X_train) \n",
    "                y_test_pred = search.predict(X_test)\n",
    "\n",
    "                model_train_predictions[str(scorer)] = y_train_pred\n",
    "                model_test_predictions[str(scorer)] = y_test_pred\n",
    "\n",
    "                y_test_prob = search.predict_proba(X_test)[:, 1]\n",
    "\n",
    "                evaluation = evaluate_model_predictions(y_test, y_test_pred, y_test_prob)\n",
    "\n",
    "                print(evaluation)\n",
    "                new_row = {\n",
    "                    'file': file,\n",
    "                    'run': run, \n",
    "                    'validation_process': VALIDATION_PROCESS, \n",
    "                    'model' : model_name,\n",
    "                    'CV': 5, \n",
    "                    'search': 'Grid',\n",
    "                    'scorer': str(scorer)\n",
    "                }\n",
    "                new_row.update(evaluation)\n",
    "                new_row['best_params'] = str(search.best_params_)\n",
    "                model_performance.append(new_row)\n",
    "                project_performance.append(new_row)\n",
    "                results.append(new_row)\n",
    "            \n",
    "        \n",
    "            train_div_scores = compute_model_diversities(model_train_predictions, y_train, scorers = SCORERS)\n",
    "            test_div_scores = compute_model_diversities(model_test_predictions, y_test, scorers = SCORERS)\n",
    "            fig_train, ax_train = plt.subplots(figsize = (10, 10))\n",
    "            fig_test, ax_test = plt.subplots(figsize = (10, 10))\n",
    "            sns.heatmap(train_div_scores.pivot(index= \"scorer_1\", columns=\"scorer_2\", values=\"diversity\"),ax=ax_train, annot=True)\n",
    "            sns.heatmap(test_div_scores.pivot(index= \"scorer_1\", columns=\"scorer_2\", values=\"diversity\"), ax=ax_test, annot=True)\n",
    "            os.makedirs(os.path.join(figures_folder, f'run_{run}', file.replace('.csv', ''),f'{model_name}'), exist_ok=True)\n",
    "            fig_train.tight_layout()\n",
    "            fig_test.tight_layout()\n",
    "            fig_train.savefig(os.path.join(figures_folder, f'run_{run}', file.replace('.csv', ''), f'{model_name}', 'div_train.png'))\n",
    "            fig_test.savefig(os.path.join(figures_folder,f'run_{run}', file.replace('.csv', ''), f'{model_name}', 'div_test.png'))\n",
    "            test_div_scores.to_csv(os.path.join(figures_folder, f'run_{run}', file.replace('.csv', ''), f'{model_name}', 'div_test.csv'), index=False)\n",
    "            train_div_scores.to_csv(os.path.join(figures_folder, f'run_{run}', file.replace('.csv', ''), f'{model_name}', 'div_train.csv'), index=False)\n",
    "            pd.DataFrame(model_performance).to_csv(os.path.join(figures_folder, f'run_{run}', file.replace('.csv', ''), f'{model_name}', f'run-{run}_{file.replace(\".csv\", \"\")}_{model_name}_performance.csv'), index=False)\n",
    "            \n",
    "            file_models_train_predictions[model_name] = model_train_predictions\n",
    "            file_models_test_predictions[model_name] = model_test_predictions\n",
    "        all_models_train_predictions[file.replace('.csv', '')] = file_models_train_predictions\n",
    "        all_models_test_predictions[file.replace('.csv', '')] = file_models_test_predictions\n",
    "\n",
    "        all_models_train_diversities = compute_all_models_diversities(file_models_train_predictions, models= PARAMETERS_GRID.keys(), y_true=y_train)\n",
    "        all_models_test_diversities = compute_all_models_diversities(file_models_test_predictions, models= PARAMETERS_GRID.keys(),  y_true=y_test)\n",
    "       \n",
    "        fig_all_models_train, ax_all_models_train = plt.subplots(figsize = (40, 40))\n",
    "        fig_all_models_test, ax_all_models_test = plt.subplots(figsize = (40, 40))\n",
    "\n",
    "        sns.heatmap(all_models_train_diversities.pivot(index= \"scorer_1\", columns=\"scorer_2\", values=\"diversity\"),ax=ax_all_models_train, annot=True)\n",
    "        sns.heatmap(all_models_test_diversities.pivot(index= \"scorer_1\", columns=\"scorer_2\", values=\"diversity\"), ax=ax_all_models_test, annot=True)\n",
    "\n",
    "        fig_all_models_train.tight_layout()\n",
    "        fig_all_models_test.tight_layout()\n",
    "        fig_all_models_train.savefig(os.path.join(figures_folder, f'run_{run}', file.replace('.csv', ''), 'div_train_all.png'))\n",
    "        fig_all_models_test.savefig(os.path.join(figures_folder, f'run_{run}', file.replace('.csv', ''), 'div_test_all.png'))\n",
    "        all_models_train_diversities.to_csv(os.path.join(figures_folder, f'run_{run}', file.replace('.csv', ''), 'div_train_all.csv'), index=False)\n",
    "        all_models_test_diversities.to_csv(os.path.join(figures_folder, f'run_{run}', file.replace('.csv', ''), 'div_test_all.csv'), index=False)\n",
    "        pd.DataFrame(project_performance).to_csv(os.path.join(figures_folder, f'run_{run}', file.replace('.csv', ''), f'run-{run}_{file.replace(\".csv\", \"\")}_performance.csv'), index=False)\n",
    "\n",
    "\n",
    "    experiment_metadata['IS_DONE'] = True \n",
    "    joblib.dump(experiment_metadata, os.path.join(RESULTS_PATH, 'experiment_' + experiment_hash, 'METADATA.joblib'))\n",
    "    final_results = pd.DataFrame(results)\n",
    "    final_results.to_csv(os.path.join(RESULTS_PATH, 'experiment_' + experiment_hash, 'results.csv'), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.5 ('Model_Selection_Issue')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7b114b69e606caf58a03deec8241f1f0c0a72b042e593cb299b8570180f33eff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
