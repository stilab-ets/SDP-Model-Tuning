{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd \n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib.backends.backend_pdf import PdfPages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#globals\n",
    "DATA_PATH = '../results/stats_results'\n",
    "\n",
    "ALGOS = ['RF', 'DT', 'KNN','FFT']\n",
    "SCORERS = ['MCC','Gmean', 'F1', 'F1_W',  'ACC', 'BAL_ACC',  'AUC', 'LogLoss']\n",
    "STUDIED_RELEASES = {   \n",
    "    'ActiveMQ': [\n",
    "        \"activemq-5.0.0\", 'activemq-5.1.0', \"activemq-5.2.0\",'activemq-5.3.0', 'activemq-5.8.0'\n",
    "        ],\n",
    "    'Derby': [\n",
    "        'derby-10.2.1.6', 'derby-10.3.1.4', 'derby-10.5.1.1'\n",
    "    ],\n",
    "    'Groovy': [\n",
    "        'groovy-1_5_7', 'groovy-1_6_BETA_1', 'groovy-1_6_BETA_2'\n",
    "    ],\n",
    "    'Hbase': [\n",
    "         'hbase-0.94.0', 'hbase-0.95.0', 'hbase-0.95.2'\n",
    "    ],\n",
    "    'Hive': [\n",
    "        'hive-0.9.0', 'hive-0.10.0', 'hive-0.12.0'\n",
    "    ],\n",
    "    'Jruby':[\n",
    "        'jruby-1.1', 'jruby-1.4.0', 'jruby-1.5.0', 'jruby-1.7.0'\n",
    "    ],\n",
    "    'Lucene': [\n",
    "        'lucene-2.3.0','lucene-2.9.0', 'lucene-3.0.0', 'lucene-3.1.0'\n",
    "    ],\n",
    "    'Wicket': [\n",
    "        'wicket-1.3.0-incubating-beta-1', 'wicket-1.3.0-beta2', 'wicket-1.5.3'\n",
    "    ]\n",
    "}\n",
    "ALL_STUDIED_RELEASES = [\n",
    "     \"activemq-5.0.0\", 'activemq-5.1.0', \"activemq-5.2.0\",'activemq-5.3.0',\n",
    "     'derby-10.2.1.6', 'derby-10.3.1.4', 'groovy-1_5_7', 'groovy-1_6_BETA_1',\n",
    "     'hbase-0.94.0', 'hbase-0.95.0',  'hive-0.9.0', 'hive-0.10.0', \n",
    "     'jruby-1.1', 'jruby-1.4.0', 'jruby-1.5.0', 'lucene-2.3.0','lucene-2.9.0',\n",
    "    'lucene-3.0.0','wicket-1.3.0-incubating-beta-1', 'wicket-1.3.0-beta2'\n",
    "]\n",
    "DATA = {\n",
    "    \n",
    "    'MCC': {\n",
    "        'data': pd.concat([pd.read_csv(os.path.join(DATA_PATH, 'MCC_results.csv')), pd.read_csv(os.path.join(DATA_PATH, 'MCC_FFT_results.csv'))]),\n",
    "        'round_digits' :3\n",
    "    },\n",
    "    'G': {\n",
    "        'data': pd.concat([pd.read_csv(os.path.join(DATA_PATH, 'G_results.csv')), pd.read_csv(os.path.join(DATA_PATH, 'G_FFT_results.csv'))]),\n",
    "        'round_digits' :3\n",
    "    },\n",
    "    'F1': {\n",
    "        'data': pd.concat([pd.read_csv(os.path.join(DATA_PATH, 'F1_results.csv')), pd.read_csv(os.path.join(DATA_PATH, 'G_FFT_results.csv'))]),\n",
    "        'round_digits' :3\n",
    "    },\n",
    "    'AUC': {\n",
    "        'data': pd.read_csv(os.path.join(DATA_PATH, 'AUC_results.csv')),\n",
    "        'round_digits' :3\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helpers\n",
    "def RQ1_tables(metrics_dfs,orgs = STUDIED_RELEASES,\n",
    "               algos = ALGOS,\n",
    "               scorers = SCORERS\n",
    "               ) : \n",
    "    \n",
    "    odd = 1\n",
    "    for org, releases in orgs.items(): \n",
    "        org_lines = []\n",
    "        #print('org:',org,'=====================================================')\n",
    "        print(f'\\\\multirow{{{len(algos)*(len(releases) - 1)}}}{{2cm}}{{{org}}}')\n",
    "        for release_index, release in enumerate(releases[:-1]):\n",
    "            print(f'&\\\\multirow{{{len(algos)}}}{{2cm}}{{{extract_release(release)}}}&\\\\multirow{{{len(algos)}}}{{2cm}}{{{extract_release(releases[release_index + 1])}}}')\n",
    "            \n",
    "            for index,algo in enumerate(algos): \n",
    "                odd =1-odd\n",
    "                if index == 0 :\n",
    "                    line = \"&\"\n",
    "                else: \n",
    "                    line = \"&&&\"\n",
    "                if odd == 1: \n",
    "                    line +=\"\\\\rowcolor[HTML]{DADADA}\"\n",
    "                line +=algo+'&'\n",
    "                for index,(metric,metric_data) in enumerate(metrics_dfs.items()) :\n",
    "                    for scorer_index, scorer in enumerate(scorers):\n",
    "                        model_metric = select_algo_performance(metric_data['data'],release,algo, scorer=scorer) \n",
    "                        item = '-'\n",
    "                        if not (model_metric is None):\n",
    "\n",
    "                            item = f'{round(model_metric[\"mean\"],metric_data[\"round_digits\"])} ({model_metric[\"group\"]})'\n",
    "                            if model_metric['group'] == 1 : \n",
    "                                item = f'\\\\textbf{{{item}}}'\n",
    "                                \n",
    "                        if index == len(metrics_dfs)-1 and  scorer_index == len(scorers) - 1: \n",
    "                            item += \"\\\\\\\\\"\n",
    "                        else :\n",
    "                            item+= \"&\"\n",
    "                        line +=item\n",
    "                org_lines.append(line)\n",
    "                print(line)\n",
    "            print('\\\\cline{2-36}')\n",
    "        print('\\\\hline')\n",
    "\n",
    "def compute_improvements(data, studied_scorer, studied_files=ALL_STUDIED_RELEASES, studied_algos = ALGOS): \n",
    "    res = {}\n",
    "    for algo in  studied_algos: \n",
    "        res[algo] =  []\n",
    "        algo_data = data[data['model'] == algo]\n",
    "        for file in studied_files: \n",
    "            file_data = algo_data[algo_data['file'] == file]\n",
    "            best_performance = file_data[file_data['group'] == 1]\n",
    "            best_file_data_scorers = file_data[file_data['group'] == 1]['scorer'].unique()\n",
    "            if studied_scorer in best_file_data_scorers: \n",
    "                continue \n",
    "\n",
    "            else : \n",
    "                best_score = best_performance.to_dict(orient='records')[0]['mean']\n",
    "                best_scorer =  best_performance.to_dict(orient='records')[0]['scorer']\n",
    "                scorer_score = file_data[file_data['scorer'] == studied_scorer].to_dict(orient='records')[0]['mean']\n",
    "                if scorer_score <= 0 : \n",
    "                    best_score += abs(scorer_score) + 2\n",
    "                    scorer_score = 2\n",
    "                res[algo].append(round((best_score - scorer_score)*100/scorer_score, 2))\n",
    "    \n",
    "    return res \n",
    "\n",
    "            \n",
    "\n",
    "def plot_metric_corrolation(dfs, files, scorer, algo, ax= None, cbar = True): \n",
    "    corrolation_map = metric_corrolation(dfs, files, scorer, algo)\n",
    "    print(corrolation_map)\n",
    "    sns.heatmap(corrolation_map, annot=True, ax=ax,  vmin=-1, vmax = 1, cbar = cbar)\n",
    "    \n",
    "def metric_corrolation(dfs, files, scorer, algo): \n",
    "    metric_ranks_df = performance_metric_corrolation(dfs, files, scorer, algo)\n",
    "    correlation_matrix = metric_ranks_df.corr(method='spearman')\n",
    "    return correlation_matrix\n",
    "\n",
    "def performance_metric_corrolation(dfs, files, scorer, algo) : \n",
    "    metric_ranks = {}\n",
    "    for metric, metric_data in dfs.items(): \n",
    "        print(metric_data)\n",
    "        selected_data = metric_data[(metric_data['file'].isin(files)) & (metric_data['scorer'] == scorer) & (metric_data['model'] == algo)]\n",
    "        selected_data = selected_data.sort_values(by = [\"file\"])\n",
    "        print(selected_data)\n",
    "        metric_ranks[metric] = selected_data['group'].values\n",
    "    return pd.DataFrame(metric_ranks)\n",
    "\n",
    "def scatter_ranks_plot(df, scorerX, scorerY, algo='RF', ax=None):\n",
    "    df_cp = df[(df['scorer'].isin([scorerX, scorerY]))&(df['model'] == algo)]\n",
    "    df_cp = df_cp.sort_values(by=['file'])\n",
    "    scorerX_ranks = df_cp[df_cp['scorer'] == scorerX]['group'].astype(int).values\n",
    "    scorerY_ranks = df_cp[df_cp['scorer'] == scorerY]['group'].astype(int).values\n",
    "    #fig, my_ax=plt.subplots()\n",
    "    df_count = pd.DataFrame({\n",
    "        scorerX + \" tuning metric ranks\": scorerX_ranks,\n",
    "        scorerY + \" tuning metric ranks\": scorerY_ranks\n",
    "    })\n",
    "    df_count= df_count.groupby([scorerX + \" tuning metric ranks\", scorerY + \" tuning metric ranks\"]).size().reset_index(name='Count')\n",
    "    print(df_count)\n",
    "    sns.scatterplot(data=df_count, x=scorerX + \" tuning metric ranks\", y=scorerY + \" tuning metric ranks\", size= \"Count\", ax=ax)\n",
    "\n",
    "\n",
    "def plot_per_metric(metric, df, studied_files=ALL_STUDIED_RELEASES, studied_algos = ALGOS, studied_scorers=SCORERS):\n",
    "    fig, axs = plt.subplots(nrows = 1, ncols = len(studied_algos),  figsize = (25, 5))\n",
    "    for algo_index, algo in enumerate(studied_algos): \n",
    "        cbar= False\n",
    "        if algo_index == len(studied_algos) - 1:\n",
    "            cbar= True\n",
    "        if algo == \"FFT\":\n",
    "            plot_corrolation(df, studied_files, [algo], ['MCC', 'F1', 'F1_W', 'ACC', 'Gmean', 'ACC', 'BAL_ACC'], axs[algo_index], vmin=-1, vmax=1, cbar = cbar)    \n",
    "        else:\n",
    "            plot_corrolation(df, studied_files, [algo], studied_scorers, axs[algo_index], vmin=-1, vmax=1, cbar = cbar)\n",
    "        axs[algo_index].set_title(algo)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_corrolation(df, studied_files=ALL_STUDIED_RELEASES, studied_algos = ALGOS, studied_scorers=SCORERS, ax=None, vmin=0, vmax = 16, cbar =False): \n",
    "    corr_matrix = comput_corrolation(df, studied_files, studied_algos, studied_scorers)\n",
    "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "    sns.heatmap(corr_matrix, annot=True, ax=ax,  vmin=vmin, vmax = vmax, cbar =cbar)\n",
    "\n",
    "    \n",
    "def comput_corrolation(df, studied_files=ALL_STUDIED_RELEASES, studied_algos = ALGOS, studied_scorers=SCORERS): \n",
    "    selected_df = df[(df[\"file\"].isin(studied_files)) & df['model'].isin(studied_algos)]\n",
    "    selected_df = selected_df.sort_values(by = ['file', 'model'])\n",
    "    corr_df = {}\n",
    "    for scorer in studied_scorers: \n",
    "        corr_df[scorer] = selected_df[selected_df['scorer'] == scorer]['group'].values\n",
    "    print(corr_df)\n",
    "    corr_df = pd.DataFrame(corr_df)\n",
    "    correlation_matrix = corr_df.corr(method='spearman')\n",
    "    return correlation_matrix\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot_metrics_scorers_exclusive_heatmaps(dfs, algos, scorers, ax, vmin=0, vmax = 16, cbar =True): \n",
    "    heatmap_df = not_exclusive_better_metric(dfs, algos=algos, scorers=scorers).pivot(index=\"Metric\", columns=\"Scorer\", values=\"Count\")\n",
    "    heatmap_df.index = pd.CategoricalIndex(heatmap_df.index, categories= scorers)\n",
    "    heatmap_df.sort_index(level=0, inplace=True)\n",
    "    #heatmap_df.sort_index(level=0, ascending=True, inplace=True)\n",
    "    \n",
    "    sns.heatmap(heatmap_df, annot=True, ax=ax, vmin = vmin, vmax=vmax, cbar=cbar)\n",
    "    ax.set_xlabel(\"Tuning metric\")\n",
    "    ax.set_ylabel(\"Performance metric\")\n",
    "\n",
    "def not_exclusive_better_metric(dfs, scorers, algos): \n",
    "    heatmap_dict = {metric : {scorer : 0 for scorer in scorers} for metric in dfs}\n",
    "    for metric, df in dfs.items(): \n",
    "        selected_df = df[(df['model'].isin(algos)) & (df['scorer'].isin(scorers))]\n",
    "        files = selected_df['file'].unique()\n",
    "        for file in files: \n",
    "            file_data = selected_df[selected_df['file'] == file]\n",
    "            for row_idx, row in file_data.iterrows(): \n",
    "                if row['group'] == 1:\n",
    "                    heatmap_dict[metric][row['scorer'] ] += 1 \n",
    "          \n",
    "    heatmap_df = []\n",
    "    for metric in dfs:\n",
    "        for scorer in scorers:\n",
    "            heatmap_df.append({\n",
    "                'Metric': metric, \n",
    "                'Scorer': scorer,\n",
    "                'Count': heatmap_dict[metric][scorer]\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(heatmap_df)\n",
    "\n",
    "def exclusive_better_metric(dfs, scorers, algos): \n",
    "    heatmap_dict = {metric : {scorer : 0 for scorer in scorers} for metric in dfs}\n",
    "    for metric, df in dfs.items(): \n",
    "        selected_df = df[(df['model'].isin(algos)) & (df['scorer'].isin(scorers))]\n",
    "        files = selected_df['file'].unique()\n",
    "        for file in files: \n",
    "            file_data = selected_df[selected_df['file'] == file]\n",
    "            best_scorers = []\n",
    "            scorer_is_metric = False\n",
    "            for row_idx, row in file_data.iterrows(): \n",
    "                if row['group'] == 1:\n",
    "                    if row['scorer'] ==  metric:\n",
    "                        scorer_is_metric=True \n",
    "                        break\n",
    "                    else: \n",
    "                        best_scorers.append(row['scorer'])\n",
    "                \n",
    "            if scorer_is_metric: \n",
    "                heatmap_dict[metric][metric] += 1 \n",
    "            else :\n",
    "                for best_scorer in best_scorers: \n",
    "                    heatmap_dict[metric][best_scorer] += 1 \n",
    "    \n",
    "    heatmap_df = []\n",
    "    for metric in dfs:\n",
    "        for scorer in scorers:\n",
    "            heatmap_df.append({\n",
    "                'Metric': metric, \n",
    "                'Scorer': scorer,\n",
    "                'Count': heatmap_dict[metric][scorer]\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(heatmap_df)\n",
    "    \n",
    "def plot_metrics_scorers_heatmaps(dfs, algos, scorers): \n",
    "    heatmap_df = compute_metrics_scorers_heatmap(dfs, models=algos, scorers=scorers).pivot(index=\"Scorer\", columns=\"Metric\", values=\"Count\")\n",
    "    heatmap_df.sort_index(level=0, ascending=True, inplace=True)\n",
    "\n",
    "    print(heatmap_df)\n",
    "    sns.heatmap(heatmap_df, annot=True)\n",
    "\n",
    "def compute_metrics_scorers_heatmap(dfs, scorers=SCORERS, models = ALGOS): \n",
    "    heatmap_dict = {metric : {scorer : 0 for scorer in scorers} for metric in dfs}\n",
    "    for metric, df in dfs.items(): \n",
    "        selected_df = df[df['model'].isin(models)]\n",
    "        selected_df = selected_df[selected_df['scorer'].isin(scorers)]\n",
    "        print(selected_df)\n",
    "        for row_ind, row in selected_df.iterrows(): \n",
    "            if row['group']==1: \n",
    "                 heatmap_dict[metric][row['scorer']] +=1\n",
    "    \n",
    "    heatmap_df = []\n",
    "    for metric in dfs:\n",
    "        for scorer in scorers:\n",
    "            heatmap_df.append({\n",
    "                'Metric': metric, \n",
    "                'Scorer': scorer,\n",
    "                'Count': heatmap_dict[metric][scorer]\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(heatmap_df)\n",
    "    \n",
    "\n",
    "\n",
    "def plot_algo_scorers_heatmap(data, algos, scorers): \n",
    "    heatmap_df = compute_algo_scorers_heatmap(data, algos, scorers).pivot(index=\"Scorer\", columns=\"Model\", values=\"Count\")\n",
    "    sns.heatmap(heatmap_df, annot=True)\n",
    "\n",
    "def select_algo_performance(df,file,algo, scorer) : \n",
    "  \n",
    "    selected_row=df[(df['file'] == file) & (df['model'] == algo) & (df['scorer'] == scorer)].to_dict(orient='records')\n",
    "    if len(selected_row) == 0:\n",
    "        return None \n",
    "    return selected_row[0]\n",
    "\n",
    "def extract_release(release_name):\n",
    "    if release_name == 'wicket-1.3.0-incubating-beta-1': \n",
    "        return '1.3.B1'\n",
    "\n",
    "    if  release_name ==  'wicket-1.3.0-beta2':\n",
    "        return '1.3.B2'\n",
    "    \n",
    "    project_name, release =  release_name.split('-')\n",
    "    return release.replace('_','.').replace('BETA', 'B').replace('B.1', 'B1').replace('B.2','B2')\n",
    "\n",
    "def compute_algo_scorers_heatmap(data, algos, scorers): \n",
    "    heatmap = {algo:{scorer: None for scorer in scorers} for algo in algos}\n",
    "    for row_index, row in data.iterrows():\n",
    "        algo = row['model']\n",
    "        scorer = row['scorer']\n",
    "        if heatmap[algo][scorer] == None:\n",
    "            heatmap[algo][scorer] = 0\n",
    "        if row['group'] == 1:\n",
    "            heatmap[algo][scorer] += 1\n",
    "    \n",
    "    heatmap_df = []\n",
    "    for algo in algos:\n",
    "        for scorer in scorers: \n",
    "            heatmap_df.append({\n",
    "                \"Model\": algo, \n",
    "                \"Scorer\": scorer,\n",
    "                \"Count\": heatmap[algo][scorer]\n",
    "            })\n",
    "    return pd.DataFrame(heatmap_df)\n",
    "\n",
    "def label_metadata(dfs,studied_files = STUDIED_RELEASES, studied_models=ALGOS, studied_scorers=SCORERS): \n",
    "    labels = []\n",
    "    selected_dfs = {metric: df[(df['model'].isin(studied_models)) & (df['scorer'].isin(studied_models))] for metric, df in dfs.items() }\n",
    "    for file in studied_files:\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_improvements = {\n",
    "    'MCC': compute_improvements(DATA['MCC']['data'], 'MCC'),\n",
    "    'G' : compute_improvements(DATA['G']['data'], 'Gmean'),\n",
    "    'F1' :  compute_improvements(DATA['F1']['data'], 'F1'),\n",
    "    'AUC' :  compute_improvements(DATA['AUC']['data'], 'AUC', studied_algos=['RF', 'DT', 'KNN']),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MCC': {'RF': [(25.0, 'AUC', 'activemq-5.0.0'),\n",
       "   (18.75, 'BAL_ACC', 'activemq-5.2.0'),\n",
       "   (4.35, 'LogLoss', 'activemq-5.3.0'),\n",
       "   (12.2, 'LogLoss', 'derby-10.2.1.6'),\n",
       "   (5.0, 'AUC', 'derby-10.3.1.4'),\n",
       "   (13.79, 'F1', 'groovy-1_5_7'),\n",
       "   (15.62, 'AUC', 'hbase-0.94.0'),\n",
       "   (10.0, 'BAL_ACC', 'hbase-0.95.0'),\n",
       "   (3.7, 'LogLoss', 'hive-0.10.0'),\n",
       "   (4.44, 'Gmean', 'jruby-1.1'),\n",
       "   (11.11, 'AUC', 'jruby-1.4.0'),\n",
       "   (6.67, 'BAL_ACC', 'jruby-1.5.0'),\n",
       "   (20.69, 'LogLoss', 'lucene-2.3.0'),\n",
       "   (2.56, 'LogLoss', 'lucene-2.9.0'),\n",
       "   (26.67, 'AUC', 'lucene-3.0.0')],\n",
       "  'DT': [(36.67, 'AUC', 'activemq-5.1.0'),\n",
       "   (15.62, 'BAL_ACC', 'activemq-5.2.0'),\n",
       "   (13.5, 'F1', 'activemq-5.3.0'),\n",
       "   (10.53, 'LogLoss', 'derby-10.2.1.6'),\n",
       "   (9.37, 'F1', 'derby-10.3.1.4'),\n",
       "   (59.09, 'LogLoss', 'groovy-1_5_7'),\n",
       "   (13.89, 'AUC', 'groovy-1_6_BETA_1'),\n",
       "   (42.31, 'ACC', 'hbase-0.95.0'),\n",
       "   (8.57, 'AUC', 'hive-0.9.0'),\n",
       "   (3.85, 'ACC', 'hive-0.10.0'),\n",
       "   (4.65, 'F1_W', 'jruby-1.1'),\n",
       "   (25.81, 'ACC', 'jruby-1.4.0'),\n",
       "   (21.43, 'ACC', 'jruby-1.5.0'),\n",
       "   (10.0, 'BAL_ACC', 'lucene-2.9.0'),\n",
       "   (3.33, 'ACC', 'wicket-1.3.0-incubating-beta-1')],\n",
       "  'KNN': [(55.56, 'Gmean', 'activemq-5.1.0'),\n",
       "   (6.25, 'AUC', 'activemq-5.2.0'),\n",
       "   (17.39, 'F1_W', 'activemq-5.3.0'),\n",
       "   (8.57, 'AUC', 'derby-10.3.1.4'),\n",
       "   (47.62, 'Gmean', 'groovy-1_5_7'),\n",
       "   (9.52, 'ACC', 'groovy-1_6_BETA_1'),\n",
       "   (10.0, 'ACC', 'hbase-0.94.0'),\n",
       "   (53.33, 'Gmean', 'hive-0.10.0'),\n",
       "   (5.0, 'ACC', 'jruby-1.4.0'),\n",
       "   (33.33, 'AUC', 'jruby-1.5.0'),\n",
       "   (10.34, 'LogLoss', 'lucene-2.3.0'),\n",
       "   (107.41, 'Gmean', 'lucene-2.9.0'),\n",
       "   (80.0, 'Gmean', 'wicket-1.3.0-incubating-beta-1'),\n",
       "   (150.0, 'LogLoss', 'wicket-1.3.0-beta2')],\n",
       "  'FFT': [(13.95, 'F1', 'activemq-5.1.0'),\n",
       "   (3.13, 'ACC', 'activemq-5.2.0'),\n",
       "   (32.0, 'ACC', 'activemq-5.3.0'),\n",
       "   (2.94, 'F1', 'hive-0.9.0'),\n",
       "   (2.38, 'ACC', 'jruby-1.1'),\n",
       "   (23.53, 'F1_W', 'jruby-1.4.0'),\n",
       "   (57.89, 'Gmean', 'jruby-1.5.0'),\n",
       "   (42.86, 'F1_W', 'lucene-2.9.0'),\n",
       "   (12.5, 'F1_W', 'lucene-3.0.0'),\n",
       "   (15.79, 'Gmean', 'wicket-1.3.0-beta2')]},\n",
       " 'G': {'RF': [(102.94, 'AUC', 'activemq-5.0.0'),\n",
       "   (1.37, 'BAL_ACC', 'activemq-5.1.0'),\n",
       "   (1.35, 'BAL_ACC', 'activemq-5.2.0'),\n",
       "   (5.8, 'MCC', 'activemq-5.3.0'),\n",
       "   (1.37, 'MCC', 'derby-10.3.1.4'),\n",
       "   (1.52, 'BAL_ACC', 'hbase-0.95.0'),\n",
       "   (2.7, 'MCC', 'hive-0.9.0'),\n",
       "   (7.25, 'LogLoss', 'hive-0.10.0'),\n",
       "   (5.13, 'MCC', 'jruby-1.4.0'),\n",
       "   (1.28, 'BAL_ACC', 'jruby-1.5.0'),\n",
       "   (9.23, 'LogLoss', 'lucene-2.3.0'),\n",
       "   (8.22, 'MCC', 'lucene-2.9.0'),\n",
       "   (2.7, 'AUC', 'wicket-1.3.0-beta2')],\n",
       "  'DT': [(12.68, 'AUC', 'activemq-5.1.0'),\n",
       "   (4.05, 'BAL_ACC', 'activemq-5.2.0'),\n",
       "   (7.35, 'F1', 'activemq-5.3.0'),\n",
       "   (2.82, 'LogLoss', 'derby-10.2.1.6'),\n",
       "   (7.35, 'F1', 'derby-10.3.1.4'),\n",
       "   (27.12, 'LogLoss', 'groovy-1_5_7'),\n",
       "   (2.78, 'LogLoss', 'groovy-1_6_BETA_1'),\n",
       "   (8.45, 'AUC', 'hive-0.9.0'),\n",
       "   (5.88, 'ACC', 'hive-0.10.0'),\n",
       "   (5.8, 'AUC', 'jruby-1.1'),\n",
       "   (3.85, 'F1', 'jruby-1.4.0'),\n",
       "   (1.28, 'BAL_ACC', 'jruby-1.5.0'),\n",
       "   (1.35, 'BAL_ACC', 'lucene-2.9.0'),\n",
       "   (8.93, 'BAL_ACC', 'lucene-3.0.0'),\n",
       "   (1.35, 'MCC', 'wicket-1.3.0-beta2')],\n",
       "  'KNN': [(1.69, 'F1', 'activemq-5.2.0'),\n",
       "   (2.86, 'MCC', 'derby-10.2.1.6'),\n",
       "   (6.12, 'MCC', 'hbase-0.94.0'),\n",
       "   (7.14, 'ACC', 'jruby-1.4.0'),\n",
       "   (21.95, 'AUC', 'jruby-1.5.0')],\n",
       "  'FFT': [(4.76, 'MCC', 'groovy-1_5_7'),\n",
       "   (8.45, 'MCC', 'hive-0.10.0'),\n",
       "   (4.23, 'F1_W', 'lucene-2.9.0'),\n",
       "   (5.45, 'BAL_ACC', 'lucene-3.0.0')]},\n",
       " 'F1': {'RF': [(258.33, 'AUC', 'activemq-5.0.0'),\n",
       "   (10.0, 'BAL_ACC', 'activemq-5.2.0'),\n",
       "   (4.92, 'Gmean', 'derby-10.2.1.6'),\n",
       "   (8.57, 'BAL_ACC', 'groovy-1_5_7'),\n",
       "   (15.91, 'Gmean', 'hbase-0.94.0'),\n",
       "   (9.52, 'MCC', 'hive-0.9.0'),\n",
       "   (16.67, 'Gmean', 'jruby-1.1'),\n",
       "   (11.11, 'LogLoss', 'jruby-1.4.0'),\n",
       "   (8.89, 'LogLoss', 'lucene-2.3.0'),\n",
       "   (10.53, 'MCC', 'lucene-2.9.0'),\n",
       "   (10.53, 'LogLoss', 'lucene-3.0.0')],\n",
       "  'DT': [(55.17, 'AUC', 'activemq-5.1.0'),\n",
       "   (5.0, 'BAL_ACC', 'activemq-5.2.0'),\n",
       "   (3.45, 'ACC', 'activemq-5.3.0'),\n",
       "   (3.33, 'LogLoss', 'derby-10.2.1.6'),\n",
       "   (5.41, 'LogLoss', 'groovy-1_5_7'),\n",
       "   (48.39, 'AUC', 'groovy-1_6_BETA_1'),\n",
       "   (4.55, 'Gmean', 'hbase-0.94.0'),\n",
       "   (4.76, 'AUC', 'hive-0.9.0'),\n",
       "   (1.89, 'F1_W', 'jruby-1.1'),\n",
       "   (10.53, 'ACC', 'jruby-1.4.0'),\n",
       "   (37.5, 'ACC', 'jruby-1.5.0'),\n",
       "   (10.0, 'MCC', 'lucene-2.3.0'),\n",
       "   (11.76, 'BAL_ACC', 'lucene-2.9.0'),\n",
       "   (5.88, 'MCC', 'lucene-3.0.0'),\n",
       "   (12.12, 'ACC', 'wicket-1.3.0-incubating-beta-1'),\n",
       "   (31.25, 'ACC', 'wicket-1.3.0-beta2')],\n",
       "  'KNN': [(3.23, 'Gmean', 'activemq-5.1.0'),\n",
       "   (14.29, 'F1_W', 'activemq-5.3.0'),\n",
       "   (5.0, 'MCC', 'derby-10.2.1.6'),\n",
       "   (6.67, 'AUC', 'derby-10.3.1.4'),\n",
       "   (30.43, 'Gmean', 'groovy-1_5_7'),\n",
       "   (4.44, 'ACC', 'groovy-1_6_BETA_1'),\n",
       "   (7.69, 'Gmean', 'hbase-0.95.0'),\n",
       "   (57.89, 'Gmean', 'hive-0.10.0'),\n",
       "   (17.95, 'ACC', 'jruby-1.4.0'),\n",
       "   (26.92, 'AUC', 'jruby-1.5.0'),\n",
       "   (7.14, 'LogLoss', 'lucene-2.3.0'),\n",
       "   (5.41, 'Gmean', 'wicket-1.3.0-incubating-beta-1'),\n",
       "   (25.0, 'Gmean', 'wicket-1.3.0-beta2')],\n",
       "  'FFT': [(22.81, 'Gmean', 'activemq-5.1.0'),\n",
       "   (1.37, 'Gmean', 'activemq-5.3.0'),\n",
       "   (6.0, 'Gmean', 'derby-10.3.1.4'),\n",
       "   (36.0, 'Gmean', 'groovy-1_6_BETA_1'),\n",
       "   (7.14, 'Gmean', 'hive-0.9.0'),\n",
       "   (1.72, 'Gmean', 'jruby-1.1'),\n",
       "   (11.59, 'Gmean', 'jruby-1.5.0'),\n",
       "   (4.23, 'F1_W', 'lucene-2.9.0'),\n",
       "   (38.1, 'BAL_ACC', 'lucene-3.0.0'),\n",
       "   (28.07, 'Gmean', 'wicket-1.3.0-incubating-beta-1'),\n",
       "   (94.87, 'Gmean', 'wicket-1.3.0-beta2')]},\n",
       " 'AUC': {'RF': [(1.19, 'LogLoss', 'activemq-5.2.0'),\n",
       "   (1.19, 'Gmean', 'activemq-5.3.0'),\n",
       "   (2.47, 'LogLoss', 'derby-10.2.1.6'),\n",
       "   (1.22, 'F1_W', 'derby-10.3.1.4'),\n",
       "   (4.76, 'MCC', 'groovy-1_6_BETA_1'),\n",
       "   (1.43, 'Gmean', 'hbase-0.95.0'),\n",
       "   (1.23, 'LogLoss', 'hive-0.10.0'),\n",
       "   (1.3, 'LogLoss', 'lucene-2.3.0'),\n",
       "   (1.2, 'Gmean', 'wicket-1.3.0-incubating-beta-1')],\n",
       "  'DT': [(37.5, 'F1', 'activemq-5.0.0'),\n",
       "   (14.08, 'LogLoss', 'activemq-5.3.0'),\n",
       "   (1.28, 'BAL_ACC', 'derby-10.2.1.6'),\n",
       "   (1.32, 'BAL_ACC', 'derby-10.3.1.4'),\n",
       "   (7.04, 'LogLoss', 'groovy-1_5_7'),\n",
       "   (2.56, 'LogLoss', 'groovy-1_6_BETA_1'),\n",
       "   (2.94, 'Gmean', 'hbase-0.94.0'),\n",
       "   (1.49, 'MCC', 'hbase-0.95.0'),\n",
       "   (1.22, 'LogLoss', 'hive-0.10.0'),\n",
       "   (2.86, 'MCC', 'lucene-3.0.0'),\n",
       "   (5.26, 'BAL_ACC', 'wicket-1.3.0-incubating-beta-1')],\n",
       "  'KNN': [(1.25, 'LogLoss', 'activemq-5.1.0'),\n",
       "   (6.85, 'F1_W', 'activemq-5.3.0'),\n",
       "   (2.63, 'LogLoss', 'groovy-1_5_7'),\n",
       "   (1.28, 'LogLoss', 'lucene-2.3.0'),\n",
       "   (1.41, 'LogLoss', 'lucene-3.0.0'),\n",
       "   (2.9, 'ACC', 'wicket-1.3.0-beta2')]}}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCC\n",
      "RF\n",
      "2.56 % - 12.036666666666667 26.67 %\n",
      "DT\n",
      "3.33 % - 18.574666666666666 59.09 %\n",
      "KNN\n",
      "5.0 % - 42.451428571428565 150.0 %\n",
      "FFT\n",
      "2.38 % - 20.697 57.89 %\n",
      "G\n",
      "RF\n",
      "1.28 % - 11.604615384615384 102.94 %\n",
      "DT\n",
      "1.28 % - 6.735999999999998 27.12 %\n",
      "KNN\n",
      "1.69 % - 7.952 21.95 %\n",
      "FFT\n",
      "4.23 % - 5.722499999999999 8.45 %\n",
      "F1\n",
      "RF\n",
      "4.92 % - 33.17999999999999 258.33 %\n",
      "DT\n",
      "1.89 % - 15.686875 55.17 %\n",
      "KNN\n",
      "3.23 % - 16.31230769230769 57.89 %\n",
      "FFT\n",
      "1.37 % - 22.900000000000002 94.87 %\n",
      "AUC\n",
      "RF\n",
      "1.19 % - 1.7766666666666664 4.76 %\n",
      "DT\n",
      "1.22 % - 7.05 37.5 %\n",
      "KNN\n",
      "1.25 % - 2.72 6.85 %\n"
     ]
    }
   ],
   "source": [
    "for metric, met_data in metrics_improvements.items():\n",
    "    print(metric)\n",
    "    for model, model_data in  met_data.items(): \n",
    "        print(model)\n",
    "        print(min(model_data),'% -', np.mean(model_data) , max(model_data), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RQ1_tables(DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_data = DATA['MCC']['data']\n",
    "selected_row=auc_data[(auc_data['file'] == 'activemq-5.0.0') & (auc_data['model'] == 'FFT') & (auc_data['scorer'] == 'LogLoss')].to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_algo_scorers_heatmap(auc_data, algos=['RF', 'DT', 'KNN', 'FFT'], scorers=['MCC', 'Gmean', 'F1', 'F1_W', 'ACC', 'BAL_ACC', 'AUC', 'LogLoss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=4, figsize = (25, 5))\n",
    "plot_metrics_scorers_exclusive_heatmaps({\n",
    "    'AUC':  DATA['AUC']['data'],\n",
    "    'MCC' : DATA['MCC']['data'],\n",
    "    'Gmean' : DATA['G']['data'], \n",
    "    'F1' : DATA['F1']['data'],\n",
    "},algos=['RF'], scorers=['MCC', 'Gmean', 'F1','F1_W','ACC', 'BAL_ACC',  'LogLoss', 'AUC']\n",
    ", ax=axs[0], cbar=False)\n",
    "axs[0].set_title(\"RF\")\n",
    "\n",
    "plot_metrics_scorers_exclusive_heatmaps({\n",
    "    'AUC':  DATA['AUC']['data'],\n",
    "    'MCC' : DATA['MCC']['data'],\n",
    "    'Gmean' : DATA['G']['data'], \n",
    "    'F1' : DATA['F1']['data'],\n",
    "},algos=['DT'], scorers=['MCC', 'Gmean', 'F1','F1_W','ACC', 'BAL_ACC', 'LogLoss', 'AUC']\n",
    ",ax = axs[1], cbar=False)\n",
    "axs[1].set_title(\"DT\")\n",
    "axs[1].set_ylabel(\"\")\n",
    "plot_metrics_scorers_exclusive_heatmaps({\n",
    "    'AUC':  DATA['AUC']['data'],\n",
    "    'MCC' : DATA['MCC']['data'],\n",
    "    'Gmean' : DATA['G']['data'], \n",
    "    'F1' : DATA['F1']['data'],\n",
    "},algos=['KNN'], scorers=['MCC', 'Gmean', 'F1','F1_W','ACC', 'BAL_ACC', 'LogLoss', 'AUC']\n",
    ",ax = axs[2], cbar=False)\n",
    "axs[2].set_title(\"KNN\")\n",
    "axs[2].set_ylabel(\"\")\n",
    "\n",
    "\n",
    "plot_metrics_scorers_exclusive_heatmaps({\n",
    "    #'AUC':  DATA['AUC']['data'],\n",
    "    'MCC' : DATA['MCC']['data'],\n",
    "    'Gmean' : DATA['G']['data'], \n",
    "    'F1' : DATA['F1']['data'],\n",
    "},algos=['FFT'], scorers=['MCC', 'Gmean', 'F1','F1_W','ACC', 'BAL_ACC']\n",
    ",ax = axs[3])\n",
    "axs[3].set_title(\"FFT\")\n",
    "axs[3].set_ylabel(\"\")\n",
    "plt.tight_layout()\n",
    "with PdfPages('performance_tuning_metrics.pdf') as pdf:\n",
    "    pdf.savefig()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_corrolation(df=DATA['G']['data'], studied_algos=['KNN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_corrolation(df=DATA['G']['data'], studied_algos=['RF'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_MCC = plot_per_metric('MCC',DATA['MCC']['data'] )\n",
    "fig_G = plot_per_metric('G',DATA['G']['data'] )\n",
    "fig_F1 = plot_per_metric('F1',DATA['F1']['data'] )\n",
    "fig_AUC = plot_per_metric('AUC',DATA['AUC']['data'],studied_algos = ['RF', 'DT', 'KNN'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " fig, ax= plt.subplots()\n",
    " scatter_ranks_plot(DATA['F1']['data'], scorerX='BAL_ACC', scorerY='LogLoss', algo='RF', ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metric_corrolation(dfs = {\n",
    "    'MCC': DATA['MCC']['data'],\n",
    "    'F1' : DATA['F1']['data'],\n",
    "    'Gmean': DATA['G']['data']#, \n",
    "    #'AUC' : DATA['AUC']['data']\n",
    "    }\n",
    "    ,\n",
    "     scorer = 'ACC', algo='FFT', files = ALL_STUDIED_RELEASES)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "code_review_delay_prediction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
